{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc3dde5a",
      "metadata": {
        "id": "dc3dde5a"
      },
      "source": [
        "코드 베이스 : [Fine Tuning Roberta for Sentiment Analysis](https://colab.research.google.com/drive/1AZ3WtoFbM845TxZqePU7_L3ysNPF8Kaa#scrollTo=979OUro5Eac3&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac98b0e3",
      "metadata": {
        "id": "ac98b0e3"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries needed\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import transformers\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d04ba44b",
      "metadata": {
        "id": "d04ba44b"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "819b8cdc",
      "metadata": {
        "id": "819b8cdc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "\n",
        "test_list = test[\"Utterance\"].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6daeff4d",
      "metadata": {
        "id": "6daeff4d"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af93daf2",
      "metadata": {
        "id": "af93daf2"
      },
      "outputs": [],
      "source": [
        "train['Target'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76579ccb",
      "metadata": {
        "id": "76579ccb"
      },
      "outputs": [],
      "source": [
        "test_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea44f8d3",
      "metadata": {
        "id": "ea44f8d3"
      },
      "outputs": [],
      "source": [
        "train['Target'] = train['Target'].replace({'neutral':0, 'surprise':1, 'fear':2, 'sadness':3, 'joy':4, 'disgust':5, 'anger':6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ae674a",
      "metadata": {
        "id": "80ae674a"
      },
      "outputs": [],
      "source": [
        "new_df = train[['Utterance', 'Target']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad4b3df",
      "metadata": {
        "id": "5ad4b3df"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f9e0d3",
      "metadata": {
        "id": "e5f9e0d3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.Utterance\n",
        "        self.targets = self.data.Target      \n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        \n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),            \n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)            \n",
        "            } "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf044751",
      "metadata": {
        "id": "bf044751"
      },
      "outputs": [],
      "source": [
        "train_size = 0.8\n",
        "train_data=new_df.sample(frac=train_size,random_state=200)\n",
        "valid_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(valid_data.shape))\n",
        "\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "valid_set = SentimentData(valid_data, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026e9895",
      "metadata": {
        "id": "026e9895"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "valid_loader = DataLoader(valid_set, **valid_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5fb19e4",
      "metadata": {
        "id": "a5fb19e4"
      },
      "outputs": [],
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.l1 = RobertaModel.from_pretrained(\"roberta-large\")\n",
        "        self.pre_classifier = torch.nn.Linear(1024, 1024)\n",
        "        self.dropout = torch.nn.Dropout(0.25)\n",
        "        self.classifier = torch.nn.Linear(1024, 7)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253b18c0",
      "metadata": {
        "id": "253b18c0"
      },
      "outputs": [],
      "source": [
        "model = RobertaClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee1af69",
      "metadata": {
        "id": "6ee1af69"
      },
      "outputs": [],
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a31cb7ea",
      "metadata": {
        "id": "a31cb7ea"
      },
      "outputs": [],
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0710bb",
      "metadata": {
        "id": "3d0710bb"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    \n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e0be70",
      "metadata": {
        "id": "b3e0be70"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bbeafaa",
      "metadata": {
        "id": "7bbeafaa"
      },
      "outputs": [],
      "source": [
        "def valid(model, valid_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(valid_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "                \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    \n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24a9c0d",
      "metadata": {
        "id": "b24a9c0d"
      },
      "outputs": [],
      "source": [
        "acc = valid(model, valid_loader)\n",
        "print(\"Accuracy on valid data = %0.2f%%\" % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d3e910c",
      "metadata": {
        "id": "6d3e910c"
      },
      "outputs": [],
      "source": [
        "class Test_SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.Utterance\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),            \n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3302834",
      "metadata": {
        "id": "b3302834"
      },
      "outputs": [],
      "source": [
        "test = Test_SentimentData(test, tokenizer, MAX_LEN)\n",
        "test_dataloader = DataLoader(test, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7954775",
      "metadata": {
        "id": "d7954775"
      },
      "outputs": [],
      "source": [
        "def inference(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    test_predict = []\n",
        "    \n",
        "    for _, data in tqdm(enumerate(test_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "        y_pred = model(ids, mask, token_type_ids)\n",
        "        test_predict += y_pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "    \n",
        "    print('Done.')\n",
        "    \n",
        "    return test_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1398ec4f",
      "metadata": {
        "id": "1398ec4f"
      },
      "outputs": [],
      "source": [
        "finetuned_roberta_pred = inference(model, test_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5aa8705",
      "metadata": {
        "id": "a5aa8705"
      },
      "outputs": [],
      "source": [
        "for i in range(len(finetuned_roberta_pred)):\n",
        "    if int(finetuned_roberta_pred[i]) == 0 :\n",
        "        finetuned_roberta_pred[i] = 'neutral'\n",
        "    elif int(finetuned_roberta_pred[i]) == 1 :\n",
        "        finetuned_roberta_pred[i] = 'surprise'\n",
        "    elif int(finetuned_roberta_pred[i]) == 2 :\n",
        "        finetuned_roberta_pred[i] = 'fear'\n",
        "    elif int(finetuned_roberta_pred[i]) == 3 :\n",
        "        finetuned_roberta_pred[i] = 'sadness'\n",
        "    elif int(finetuned_roberta_pred[i]) == 4 :\n",
        "        finetuned_roberta_pred[i] = 'joy'\n",
        "    elif int(finetuned_roberta_pred[i]) == 5 :\n",
        "        finetuned_roberta_pred[i] = 'disgust'\n",
        "    elif int(finetuned_roberta_pred[i]) == 6 :\n",
        "        finetuned_roberta_pred[i] = 'anger'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f149c14",
      "metadata": {
        "id": "9f149c14"
      },
      "outputs": [],
      "source": [
        "finetuned_roberta_pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7945ba0",
      "metadata": {
        "id": "d7945ba0"
      },
      "outputs": [],
      "source": [
        "submit['Target'] = finetuned_roberta_pred\n",
        "submit.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bed0c7e",
      "metadata": {
        "id": "6bed0c7e"
      },
      "outputs": [],
      "source": [
        "submit.to_csv('./roberta_large_finetune.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd306d4",
      "metadata": {
        "id": "9dd306d4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dpdb_api_test",
      "language": "python",
      "name": "dpdb_api_test"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
