{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ocWL6LLqrLiD"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score,f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import transformers\n",
    "from transformers import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tCjOpb7urqtY"
   },
   "outputs": [],
   "source": [
    "train      = pd.read_csv(\"open/train_data.csv\", encoding=\"utf-8\")\n",
    "test       = pd.read_csv(\"open/test_data.csv\", encoding=\"utf-8\")\n",
    "submission = pd.read_csv(\"open/sample_submission.csv\", encoding=\"utf-8\")\n",
    "topic_dict = pd.read_csv(\"open/topic_dict.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                             title  topic_idx\n",
       "0      0          인천→핀란드 항공기 결항…휴가철 여행객 분통          4\n",
       "1      1    실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4\n",
       "2      2    이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4\n",
       "3      3  NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4\n",
       "4      4         시진핑 트럼프에 중미 무역협상 조속 타결 희망          4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape:(45654, 3)\n",
      "test.shape:(9131, 2)\n",
      "train label 개수: 7\n"
     ]
    }
   ],
   "source": [
    "print(f'train.shape:{train.shape}')\n",
    "print(f'test.shape:{test.shape}')\n",
    "print(f'train label 개수: {train.topic_idx.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',  cache_dir='bert_ckpt', do_lower_case=False)\n",
    "\n",
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    \n",
    "    encoded_dict=tokenizer.encode_plus(\n",
    "        text = sent, \n",
    "        add_special_tokens=True, \n",
    "        max_length=MAX_LEN, \n",
    "        pad_to_max_length=True, \n",
    "        return_attention_mask=True,\n",
    "        truncation = True)\n",
    "    \n",
    "    input_id=encoded_dict['input_ids']\n",
    "    attention_mask=encoded_dict['attention_mask']\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "input_ids =[]\n",
    "attention_masks =[]\n",
    "token_type_ids =[]\n",
    "train_data_labels = []\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent_clean=re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \" \", sent)\n",
    "    return sent_clean\n",
    "\n",
    "for train_sent, train_label in zip(train['title'], train['topic_idx']):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_sent), MAX_LEN=MAX_LEN)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        #########################################\n",
    "        train_data_labels.append(train_label)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n",
    "\n",
    "train_input_ids=np.array(input_ids, dtype=int)\n",
    "train_attention_masks=np.array(attention_masks, dtype=int)\n",
    "train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "###########################################################\n",
    "train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\n",
    "train_labels=np.asarray(train_data_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 계층 교차 검증\n",
    "# n_fold = 5  \n",
    "# seed = 42\n",
    "\n",
    "# cv = StratifiedKFold(n_splits = n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "# # 테스트데이터의 예측값 담을 곳 생성\n",
    "# test_y = np.zeros((test_x.shape[0], 7))\n",
    "\n",
    "# # 조기 종료 옵션 추가\n",
    "# es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "#                    verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "\n",
    "# for i, (i_trn, i_val) in enumerate(cv.split(train_x, Y_train), 1):\n",
    "#     print(f'training model for CV #{i}')\n",
    "\n",
    "#     model3.fit(train_x[i_trn], \n",
    "#             to_categorical(Y_train[i_trn]),\n",
    "#             validation_data=(train_x[i_val], to_categorical(Y_train[i_val])),\n",
    "#             epochs=10,\n",
    "#             batch_size=512,\n",
    "#             callbacks=[es])     # 조기 종료 옵션\n",
    "                      \n",
    "#     test_y += model3.predict(test_x) / n_fold    # 나온 예측값들을 교차 검증 횟수로 나눈다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids =[]\n",
    "attention_masks =[]\n",
    "token_type_ids =[]\n",
    "train_data_labels = []\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent_clean=re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \" \", sent)\n",
    "    return sent_clean\n",
    "\n",
    "for test_sent in test['title']:\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(test_sent), MAX_LEN=100)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        #########################################\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(test_sent)\n",
    "        pass\n",
    "    \n",
    "test_input_ids=np.array(input_ids, dtype=int)\n",
    "test_attention_masks=np.array(attention_masks, dtype=int)\n",
    "test_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "###########################################################\n",
    "test_inputs=(test_input_ids, test_attention_masks, test_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=np.zeros((len(test),7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf2_bert_classifier -- Folder already exists \n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000012FA625E640>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000012FA625E640>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.7773WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1142/1142 [==============================] - 551s 471ms/step - loss: 0.6657 - accuracy: 0.7773 - val_loss: 0.7432 - val_accuracy: 0.7461\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.74614, saving model to tf2_bert_classifier\\weights.h5\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 539s 472ms/step - loss: 0.4559 - accuracy: 0.8463 - val_loss: 0.6545 - val_accuracy: 0.7678\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.74614 to 0.76782, saving model to tf2_bert_classifier\\weights.h5\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 540s 473ms/step - loss: 0.3662 - accuracy: 0.8717 - val_loss: 0.6452 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.76782 to 0.77144, saving model to tf2_bert_classifier\\weights.h5\n",
      "Epoch 4/10\n",
      "1142/1142 [==============================] - 541s 473ms/step - loss: 0.2980 - accuracy: 0.8964 - val_loss: 0.6684 - val_accuracy: 0.7883\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.77144 to 0.78830, saving model to tf2_bert_classifier\\weights.h5\n",
      "Epoch 5/10\n",
      "1142/1142 [==============================] - 541s 474ms/step - loss: 0.2440 - accuracy: 0.9150 - val_loss: 0.7344 - val_accuracy: 0.7654\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.78830\n",
      "Epoch 6/10\n",
      "1142/1142 [==============================] - 541s 473ms/step - loss: 0.1923 - accuracy: 0.9332 - val_loss: 0.7715 - val_accuracy: 0.7643\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.78830\n",
      "====== predicting test===========\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "====== predicting test Done===========\n",
      "score for fold 0 is [-1.94340837 -2.1357286   0.78273344  7.23969316 -1.38593197 -1.52377498\n",
      " -2.30291629]\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "1142/1142 [==============================] - 542s 475ms/step - loss: 0.2396 - accuracy: 0.9172 - val_loss: 0.7973 - val_accuracy: 0.7691\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.78830\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 543s 475ms/step - loss: 0.1933 - accuracy: 0.9340 - val_loss: 0.8192 - val_accuracy: 0.7678\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.78830\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 542s 475ms/step - loss: 0.1562 - accuracy: 0.9465 - val_loss: 0.9273 - val_accuracy: 0.7583\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.78830\n",
      "====== predicting test===========\n",
      "====== predicting test Done===========\n",
      "score for fold 1 is [-2.81484294 -2.98099792  1.07469502 15.48944235 -3.50820041 -1.08052742\n",
      " -4.56745696]\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "1142/1142 [==============================] - 542s 475ms/step - loss: 0.1948 - accuracy: 0.9330 - val_loss: 0.7474 - val_accuracy: 0.7866\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.78830\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 543s 476ms/step - loss: 0.1600 - accuracy: 0.9442 - val_loss: 0.8798 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.78830\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 546s 478ms/step - loss: 0.1326 - accuracy: 0.9545 - val_loss: 0.8609 - val_accuracy: 0.7782\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.78830\n",
      "====== predicting test===========\n",
      "====== predicting test Done===========\n",
      "score for fold 2 is [-3.81466174 -4.94788277  2.4624882  24.08434248 -5.60893655 -1.73972327\n",
      " -6.48502314]\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "1142/1142 [==============================] - 544s 477ms/step - loss: 0.1606 - accuracy: 0.9443 - val_loss: 0.9763 - val_accuracy: 0.7584\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.78830\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 542s 475ms/step - loss: 0.1367 - accuracy: 0.9534 - val_loss: 0.9054 - val_accuracy: 0.7673\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.78830\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 543s 476ms/step - loss: 0.1137 - accuracy: 0.9621 - val_loss: 0.8450 - val_accuracy: 0.7853\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.78830\n",
      "Epoch 4/10\n",
      "1142/1142 [==============================] - 544s 477ms/step - loss: 0.0923 - accuracy: 0.9689 - val_loss: 0.9126 - val_accuracy: 0.7794\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.78830\n",
      "Epoch 5/10\n",
      "1142/1142 [==============================] - 545s 478ms/step - loss: 0.0862 - accuracy: 0.9710 - val_loss: 1.0024 - val_accuracy: 0.7663\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.78830\n",
      "====== predicting test===========\n",
      "====== predicting test Done===========\n",
      "score for fold 3 is [-4.39637232 -6.68609512  2.67685653 31.64210653 -5.83382724 -3.87158924\n",
      " -9.11520803]\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "1142/1142 [==============================] - 545s 477ms/step - loss: 0.0990 - accuracy: 0.9661 - val_loss: 0.9293 - val_accuracy: 0.7741\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.78830\n",
      "Epoch 2/10\n",
      "1142/1142 [==============================] - 545s 477ms/step - loss: 0.0886 - accuracy: 0.9701 - val_loss: 0.9457 - val_accuracy: 0.7748\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.78830\n",
      "Epoch 3/10\n",
      "1142/1142 [==============================] - 545s 477ms/step - loss: 0.0786 - accuracy: 0.9737 - val_loss: 0.9323 - val_accuracy: 0.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.78830\n",
      "Epoch 4/10\n",
      "1142/1142 [==============================] - 546s 478ms/step - loss: 0.0706 - accuracy: 0.9764 - val_loss: 1.0403 - val_accuracy: 0.7719\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.78830\n",
      "Epoch 5/10\n",
      "1142/1142 [==============================] - 546s 478ms/step - loss: 0.0672 - accuracy: 0.9775 - val_loss: 1.1847 - val_accuracy: 0.7448\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.78830\n",
      "====== predicting test===========\n",
      "====== predicting test Done===========\n",
      "score for fold 4 is [ -6.37229657  -9.10835779   2.91837014  40.7464757   -6.85529755\n",
      "  -4.23812643 -10.98394799]\n",
      "============================================================\n",
      "final LB score 0.8724027872085571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
    "                                                name=\"classifier\")\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        \n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1] \n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=7)\n",
    "\n",
    "# 학습 준비하기\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model_name = \"tf2_bert_classifier\"\n",
    "\n",
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2, restore_best_weights=True)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = os.path.join(model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "n_fold = 3\n",
    "a = 0\n",
    "cv = StratifiedKFold(n_splits = n_fold, shuffle=True)\n",
    "score=[]\n",
    "\n",
    "# 학습과 eval 시작\n",
    "for train_index, test_index in cv.split(train['title'], train['topic_idx']):\n",
    "    history = cls_model.fit(train_inputs, train_labels, epochs=10, batch_size=32,\n",
    "                            validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "    \n",
    "    print(\"====== predicting test===========\")\n",
    "    results += cls_model.predict(test_inputs)\n",
    "    \n",
    "    print(\"====== predicting test Done===========\")\n",
    "    print(f\"score for fold {a} is\",results[1])\n",
    "    \n",
    "    print(\"===\"*20)\n",
    "    score.append(results[1])\n",
    "    \n",
    "    a+=1\n",
    "    \n",
    "print(\"final LB score\",np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = cls_model.predict(test_inputs)\n",
    "results=results/5\n",
    "results=tf.argmax(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['topic_idx']=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>54780</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>54781</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9128</th>\n",
       "      <td>54782</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>54783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9130</th>\n",
       "      <td>54784</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  topic_idx\n",
       "0     45654          0\n",
       "1     45655          3\n",
       "2     45656          2\n",
       "3     45657          2\n",
       "4     45658          3\n",
       "...     ...        ...\n",
       "9126  54780          3\n",
       "9127  54781          2\n",
       "9128  54782          2\n",
       "9129  54783          2\n",
       "9130  54784          2\n",
       "\n",
       "[9131 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('3rd_bert_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EGnwGFhW-82V"
   },
   "outputs": [],
   "source": [
    "#submission.to_csv(path, index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOp+cYu5HO1Juu/TzQAEwsd",
   "collapsed_sections": [],
   "mount_file_id": "1cQiNqh_d0O6DrI3BnB8OrOzE9FiREnRm",
   "name": "KLUE_CNN.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
